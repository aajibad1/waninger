<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

{% extends 'base.html' %}


{%  block content %}
    <div id="content" class="container">
        <!-- Reddit -->
        <div class="project">
            <div class="project-title">
                <h2>Reddit Jokes</h2>
                <a href="http://redditj.lukewaninger.com" target='_blank' title="See it!">
                    <img src="{{ url_for('static', filename='aws.png') }}" alt="See it!") />
                </a>
                <a href="https://github.com/lukeWaninger/Reddit-Topic-Modeling" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
                <a href="/downloads/redditj.pdf" target="_blank" title="Project Report">
                    <img src="{{ url_for('static', filename='pdf.png') }}" alt="Project Report" />
                </a>
            </div>
            <p>
                Text data is one of the most abundant sources of data available today, yet it is one of the most
                difficult sources to understand and gain insight from. We chose to tackle this problem in a tagteam
                approach of using machine learning and effective data visualization techniques to help us understand                 the topics of a
                dataset. We used a dataset scraped from Redditâ€™s r/jokes subreddit and implemented a topic modelling
                algorithm to derive joke topics, and then implemented a visualization in d3 to maximize the ability
                of a user to understand these topics.
            </p>
            <p>
                A team of three - Mohammed Helal, Vivek Pagadala, and myself - performed this topic modeling using
                Latent Dirichlet allocation.  To do this we used a standardized 5 step approach to designing a user
                interface and experience: The Five Design-Sheet (FdS) approach for Sketching Information
                Visualization Designs by J. C. Roberts. More on this approach can be found at <a
                    href="http://pages.bangor.ac.uk/~pas601/papers/FdS-Roberts-2011.pdf" target="_blank">FDS</a>.

                The project report, source code, and final product can be accessed through the linked images above.
            </p>
        </div>

        <!--River Runners -->
        <div class="project">
            <div class="project-title">
                <h2>River Runners</h2>
                <a href="http://rr.lukewaninger.com" target='_blank' title="See it!">
                    <img src="{{ url_for('static', filename='aws.png') }}" alt="See it!") />
                </a>
                <a href="https://github.com/lukeWaninger/RiverRunner" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
            </div>
            <p>
                This project was developed by Kenten Danas, Ryan Bald, and myself as a Software Design course project.
                River Runners is a simple web-based tool that allows whitewater kayakers to see flow rate predictions
                for river runs in Washington state in order to help plan future kayaking trips. By using
                autoregressive integrated moving average (ARIMA) models on public USGS historical river flow rate
                data, along with exogenous regressors such as precipitation, snow pack, and temperature, River
                Runner provides predictions of daily average river flow rates seven days in advance. Results are
                shown on a plot along with the maximum and minimum runnable flow rates so paddlers can easily see
                whether the river is predicted to be runnable.

                Scalability was a major characteristic we kept in mind during the development of this project.
                Currently we're only predicting flow rates for the state of Washington but this can easily be
                expanded to more states as required. Feel free to send me an email if you'd like any specific
                kayaking runs added to the list!
            </p>
        </div>

        <!-- SVMs -->
        <div class="project">
            <div class="project-title">
                <h2>Kernel-Based Support Vector Machines</h2>
                <a href="https://notebooks.azure.com/lukewaninger/libraries/SupportVectorMachines" target="_blank">
                    <img src="{{ url_for('static', filename='azure.png') }}" alt="Azure Notebooks"/>
                </a>
                <a href="https://github.com/lukeWaninger/SupportVectorMachines" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
            </div>
            <p>
                The notebook, SVMKernels.ipynb, displays my implementation of a kernel-based Support Vector Machine.
                The SVM class is implemented in the notebook while supporting classes and code was implemented in
                source.py. The source code was extracted from the notebook in order to maintain readability and keep
                the notebook focused on the SVM itself rather than other required components (i.e. One vs Rest
                classifier, parallel progress bar, etc). However, I find both files quite interesting for various
                reasons.
            </p>
            <p>
                The SVM itself was implemented using the Huberized (smooth) hinge-loss and L2 regularization.


                Additionally, several kernels were implemented: Radial Bias (RBF/Gaussian), polynomial, sigmoid, and
                more. Click the Github logo to the right to see the code. Or, click the Azure logo to see the live
                notebook. It's a useful learning tool to play with the different kernels seeing which most accurately
                maps to the function being optimized. The demo also shows a comparison to scikit-learn's SVM
                implementation.
            </p>
        </div>

        <!-- Logistic Regression -->
        <div class="project">
            <div class="project-title">
                <h2>Logistic Regression</h2>
                <a href="https://notebooks.azure.com/lukewaninger/libraries/LogisticRegression" target="_blank">
                     <img src="{{ url_for('static', filename='azure.png') }}" alt="Azure Notebooks"/>
                </a>
                <a href="#" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
                <a href="/downloads/logistic_regression_proof.pdf" target="_blank" title="Project Report">
                    <img src="{{ url_for('static', filename='pdf.png') }}" alt="Project Report" />
                </a>
            </div>
            <p>
                The file logistic_regression.py is my implementation of a logistic regression classifier with L2
                regularization. The optimization problem is solved using the fast-gradient descent algorithm. The
                classifier uses the inverse of the Lipshitz constant to calculate the initial learning rate and uses
                backtracking line search (w/the Armijo stopping condition) to update the learning rate through
                successive iterations. The combination of fast-gradient and backtracking line search is shown to
                optimize much more quickly than gradient descent alone.
            </p>
            <p>
                Jupyter notebook demos for a simulated dataset and the well known Digits dataset are included as
                well as a performance comparison to scikit-learn.
            </p>
        </div>
    </div>
{% endblock %}