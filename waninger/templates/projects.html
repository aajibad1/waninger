{% extends 'base.html' %}

{%  block content %}
    <div id="content" class="container">
        <!-- Wildfires -->
        <div class="project">
            <div class="project-title">
                <h2>US Wildfire Analysis and Prediction</h2>
                <a href="https://github.com/lukeWaninger/hcds-final" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
            </div>
        </div>
        <p>
            Wildfires have been a big topic in the recent news with devastating effects across the western coast of
            the United States. So far in 2018, we have had less burn than 2017, but the current fire in California is
            the largest in state history and still burns rapidly. Last year, we had almost 2 billion dollars of
            losses across the United States as a result of wildfire damage which has been the highest in history [6]
            . Risks of wildfires continue to climb as scientists discover alarming links between rising greenhouse
            gasses, temperature, and wildfire severity. N. P. Gillett et al. performed a comprehensive study on the
            relationship between the two and concluded with overwhelming confidence that a positive trend exists
            between them [2]. Rising greenhouse gasses could be playing a significant role in the prevalence and
            severity of forest fires.

            The goal of this project is two-fold. One, to understand the independent variables and correlation
            effects in a combined dataset of the Fire Program Analysis (FPA) reporting system, NOAA's Global Surface
            Summary of Day Data (GSOD) 7, and NASA's biomass indicators. Two, to train and assess a model for
            predicting the reason a wildfire started. (and possibly estimate the impact? location?) Identifying the
            source is a difficult task for investigators in the wild. The vastness of land covered is much larger
            than the matchstick or location of a lightning strike. Developing an understanding of the independent
            variables and a reliable prediction model could give authorities valuable direction as to where to begin
            their search.
        </p>

        <!-- Reddit -->
        <div class="project">
            <div class="project-title">
                <h2>Reddit Jokes</h2>
                <a href="http://redditj.lukewaninger.com" target='_blank' title="See it!">
                    <img src="{{ url_for('static', filename='aws.png') }}" alt="See it!") />
                </a>
                <a href="https://github.com/lukeWaninger/Reddit-Topic-Modeling" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
                <a href="/downloads/redditj.pdf" target="_blank" title="Project Report">
                    <img src="{{ url_for('static', filename='pdf.png') }}" alt="Project Report" />
                </a>
            </div>
            <p>
                Text data is one of the most abundant sources of data available today, yet it is one of the most
                difficult sources to understand and gain insight from. We chose to tackle this problem in a tagteam
                approach of using machine learning and effective data visualization techniques to help us understand
                the topics of a dataset. We used a dataset scraped from Reddit’s r/jokes subreddit and implemented a
                topic modelling algorithm to derive joke topics, and then implemented a visualization in d3 to
                maximize the ability of a user to understand these topics.
            </p>
            <p>
                A team of three - <a href="https://github.com/moefasa" target="_blank">Mohammed Helal</a>, <a
                    href="https://github.com/viv-r" target="_blank">Vivek Pagadala</a>, and myself - performed this
                topic modeling using Latent Dirichlet allocation.  To do this we used a standardized 5 step approach
                to designing a user interface and experience: The Five Design-Sheet (FdS) approach for Sketching
                Information Visualization Designs by J. C. Roberts. More on this approach can be found at <a
                    href="http://pages.bangor.ac.uk/~pas601/papers/FdS-Roberts-2011.pdf" target="_blank">FDS</a>. The
                project report, source code, and final product can be accessed through the linked images above.
            </p>
        </div>

        <!--River Runners -->
        <div class="project">
            <div class="project-title">
                <h2>River Runners</h2>
{#                <a href="http://rr.lukewaninger.com" target='_blank' title="See it!">#}
{#                    <img src="{{ url_for('static', filename='aws.png') }}" alt="See it!") />#}
{#                </a>#}
                <a href="https://github.com/lukeWaninger/RiverRunner" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
            </div>
            <p>
                This project was developed by <a href="https://www.linkedin.com/in/kentendanas">Kenten Danas</a>,
                Ryan Bald, and myself as a Software Design course project. River Runners is a simple web-based tool
                that allows whitewater kayakers to see flow rate predictions for river runs in Washington state in
                order to help plan future kayaking trips. By using autoregressive integrated moving average (ARIMA)
                models on public USGS historical river flow rate data, along with exogenous regressors such as
                precipitation, snow pack, and temperature, River Runner provides predictions of daily average river
                flow rates seven days in advance. Results are shown on a plot along with the maximum and minimum
                runnable flow rates so paddlers can easily see whether the river is predicted to be runnable.
            </p>
            <p>
                For many sports that are weather-dependent, modern day weather forcasting provides relatively
                accurate predictions of conditions up to ten days in advance making it relatively easy to plan ahead
                for any outdoor activities. For whitewater kayaking however, there are currently no reliable tools
                which can predict a river’s flow rate, meaning paddlers are often left to figure out whether they
                can run a given river less than a day in advance or even the morning of their planned trip.
                Especially for newer paddlers who have fewer choices of rivers to run, or paddlers who have to plan
                ahead for traveling longer distances to get to the river, this can lead to many cancelled trips and
                dissapointing weekends without paddling. Being able to get a sense of what rivers might be runnable
                further than a day or two in advance would undoubtedly help many paddlers get the most of their time
                on the river.
            </p>
        </div>

        <!-- SVMs -->
        <div class="project">
            <div class="project-title">
                <h2>Kernel-Based Support Vector Machines</h2>
                <a href="https://notebooks.azure.com/lukewaninger/libraries/SupportVectorMachines" target="_blank">
                    <img src="{{ url_for('static', filename='azure.png') }}" alt="Azure Notebooks"/>
                </a>
                <a href="https://github.com/lukeWaninger/SupportVectorMachines" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
            </div>
            <p>
                The notebook, SVMKernels.ipynb, displays my implementation of a kernel-based Support Vector Machine.
                The SVM class is implemented in the notebook while supporting classes and code was implemented in
                source.py. The source code was extracted from the notebook in order to maintain readability and keep
                the notebook focused on the SVM itself rather than other required components (i.e. One vs Rest
                classifier, parallel progress bar, etc). However, I find both files quite interesting for various
                reasons.
            </p>
            <p>
                The SVM itself was implemented using the Huberized (smooth) hinge-loss and L2 regularization.
                Additionally, several kernels were implemented: Radial Bias (RBF/Gaussian), polynomial, sigmoid, and
                more. Click the Github logo to the right to see the code. Or, click the Azure logo to see the live
                notebook. It's a useful learning tool to play with the different kernels seeing which most accurately
                maps to the function being optimized. The demo also shows a comparison to scikit-learn's SVM
                implementation.
            </p>
        </div>

        <!-- Logistic Regression -->
        <div class="project">
            <div class="project-title">
                <h2>Logistic Regression</h2>
                <a href="https://notebooks.azure.com/lukewaninger/libraries/LogisticRegression" target="_blank">
                     <img src="{{ url_for('static', filename='azure.png') }}" alt="Azure Notebooks"/>
                </a>
                <a href="https://github.com/lukeWaninger/LogisticRegression" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
                <a href="/downloads/logistic_regression_proof.pdf" target="_blank" title="Project Report">
                    <img src="{{ url_for('static', filename='pdf.png') }}" alt="Project Report" />
                </a>
            </div>
            <p>
                The file logistic_regression.py is my implementation of a logistic regression classifier with L2
                regularization. The optimization problem is solved using the fast-gradient descent algorithm. The
                classifier uses the inverse of the Lipshitz constant to calculate the initial learning rate and uses
                backtracking line search (w/the Armijo stopping condition) to update the learning rate through
                successive iterations. The combination of fast-gradient and backtracking line search is shown to
                optimize much more quickly than gradient descent alone.
            </p>
            <p>
                Jupyter notebook demos for a simulated dataset and the well known Digits dataset are included as
                well as a performance comparison to scikit-learn. Additionally, the proof can be downloaded by
                clicking the pdf icon above.
            </p>
        </div>

        <!-- call center exploration -->
        <div class="project">
            <div class="project-title">
                <h2>Call Center Exploration</h2>
                <a href="https://notebooks.azure.com/lukewaninger/libraries/CallCenterExploration" target="_blank">
                     <img src="{{ url_for('static', filename='azure.png') }}" alt="Azure Notebooks"/>
                </a>
                <a href="https://github.com/lukeWaninger/CallCenterExploration" target="_blank" title="Github">
                    <img src="{{ url_for('static', filename='github.png') }}" alt="Github"/>
                </a>
            </div>
            <p>
                The purpose of this project was to gain insights into call center tickets and provide the leadership
                team with findings and recommendations. The overarching idea going into the project was that user
                behavior may have an impact on tickets closed through 'user education' rather than spending other
                company resources. I go through data understanding, visualization, normalization, and imputation
                followed by insights and recommendations.
            </p>
            <p>
                Additionally, I demo a generalized producer-consumer pattern of parallelization for fitting models
                and performing grid-search. The implementation of the prod-con mapping function is located within
                notebook_init.py. The power and beauty of Plotly visualizations is also demonstrated throughout the
                notebook. To view the entire notebook it must be ran with Python 3.6 in trusted mode.
            </p>
        </div>

        <!-- TEMPLATE
        <div class="project">
                <div class="project-title">
                    <h2></h2>
                    <a href="#" target="_blank">
                         <img src="" alt="Azure Notebooks"/>
                    </a>
                    <a href="#" target="_blank" title="Github">
                        <img src="" alt="Github"/>
                    </a>
                    <a href="#" target="_blank" title="Project Report">
                        <img src="" alt="Project Report" />
                    </a>
                </div>
                <p>

                </p>
            </div>
        </div>
        -->
{% endblock %}